\section{Eigenvalues, Eigenvectors, and
Invariant Subspaces}
\subsection{Invariant Subspaces}
\begin{definition}
    Let $T \in \c L(V,V)$ on a vector space $V \neq \lb 0 \rb$. A subspace $U \subseteq V$ is called an invariant subspace is invariant under $T$ if $Tu \in U \ \forall u \in U$.
\end{definition}
\begin{example}
    For any $T \in \c L(V,V)$, the following subspaces are invariant:
    \begin{enumerate}
        \item $\lb 0 \rb$
        \item $V$
        \item $\nul T = \lb v \in V : Tv = 0 \rb$ \\
        If $Tv \in \nul T$, then $Tv = 0 \in \nul T$.
        \item $\range T = \lb w \in W : w = Tv \text{ for some } v \in V \rb$ \\
        So $Tw \in \range T$.
    \end{enumerate}
\end{example}
\begin{question}
    What are $1$-dimensional invariant subspaces?  
\end{question}
\begin{answer}
Then $U = \spa(u)$ for some $u \neq 0$. Invariant means $Tu = \lambda u$ for some $\lambda \in  \b F$, where $u$ is the eigenvector of $T$ and $\lambda$ is the eigenvalues.
\end{answer}
\begin{remark}
    $u \neq 0$ if $u$ is a eigenvector is $T$. $\lambda = 0$ is possible.
\end{remark}
\begin{proposition} Let $T$ be a linear operator in $V$, then the following are equivalent
\begin{enumerate}
    \item $\lambda$ is a eigenvalue of $T$.
    \item $T - \lambda\b I$ is not invertible.
    \item $T - \lambda \b I$ is not injective.
    \item $T - \lambda \b I$ is not surjective.
\end{enumerate}
\end{proposition}
We have already proven that statement $2,3,4$ are logically equivalent. 
\begin{theorem}
    Suppose $\li vm$ are eigenvectors of $T \in \c L(V)$ corresponding to distinct eigenvalues $\li \lambda m$ will be linearly independent.
\end{theorem}
\begin{proof}
Suppose $\li vm$ are linearly independent. By linear dependence lemma, we find a the minimum index $k \leq m$ such that $v_k \in \spa (\li v{k-1})$. i.e.
\begin{equation} \label{eqn1}
     v_k = \lincomb{\alpha}{v}{k-1} 
\end{equation}
Apply linear transformation on both sides 
\begin{equation} \label{eqn2}
    Tv_k = T\lincomb{\alpha}{v}{k-1} 
\end{equation}
\begin{equation} \label{eqn3}
    \lambda v_k = \alpha_1 \lambda_1 v_1 + \alpha_2 \lambda_2 v_2 + \cdots + \alpha_n \lambda_n v_n 
\end{equation} 
We multiply by equation \ref{eqn1} by $\lambda_m$ and subtract by from \ref{eqn3} and we get \[ 0 = \alpha_1 (\lambda_1 - \lambda_k)v_1 + \alpha_2(\lambda_2 - \lambda_k)v_2 + \cdots + \alpha_{k-1} (\lambda_{k-1} - \lambda_k)v_{k-1}\]
A contradiction since $k$ is not the minimum index with the property chosen above. Therefore the list $\li vm$ must be linearly independent.
\end{proof}
\begin{corollary}
An operator $T \in \c L(V)$ has at most $\boxed{\dim V}$ distinct eigenvalues.
\end{corollary}
\subsection{Eigenvectors and Upper-Triangular
Matrices}
\subsubsection{Polynomials in T}
\begin{definition}
    Suppose $T \in \c L(V)$, then $T^k$ is defined as
    \[ T^k := \underbrace{k \circ k \circ \cdots \circ k}_{k \text{ times}}\]
    Notice that $T^0 = \b I, T^1 = T$.
\end{definition}
\begin{definition}
    If $p(x) = a_0 + a_1x + \cdots + a_nx^n$, then we can define $p(T)$ as $a_o\b I + a_1T + a_2T + \cdots + a_nT^n$.
\end{definition}
\begin{example}
    Let $V := \c P(\b R), S: p \mapsto 3p'' +  2p' + p, D: p \mapsto p'$. We can see that $S$ can be expressed as $S = D^0 + 2D + 3D^2$. Therefore \[\c M(S) = 3\c M^2(D) + 2\c M(D) + M(\b I)\] we need to have to take the same basis for inputs and output when forming $\c M(\cdot)$.  
    
    \noindent Let's use our favorite basis $1,x,x^2, x^3$. We then can see
    \[ \c M(D) = \bml 0 & 1 & 0 & 0 \\ 0 & 0 & 2 & 0 \\ 0 & 0 & 0 & 3 \\ 0 & 0 & 0 & 0 \bmr, \c M(S) = \bml 1 & 2 & 6 & 0\\ 0 & 1 & 4 & 18\\ 0 & 0 & 1 & 6 \\ 0 & 0 & 0 & 1 \bmr\]
\end{example}
\begin{question}
    What is the best matrix representation for an operator?
\end{question}
\begin{question}
    What information about eigenvalues/eigenvectors can be read off from a matrix representation?
\end{question}
\begin{theorem}
    Suppose $T \in \c L(V)$ and $\li vn$ is a basis of $V$. Then the following are logically equivalent:
    \begin{enumerate}
        \item  $\c M(T)$ is upper triangular.
        \item $Tv_j \in \spa (\li vj)$ $\forall j = 1,2, \ldots, n$.
        \item $\spa (\li vj)$ is invariant under $T$ $\forall j = 1,2,\ldots, n$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    $1) \implies 2) $ \[\bml * & * & * & * & \cdots & * \\  & * & * & * & \cdots & * \\  &  & * & * & \cdots & * \\ & & & * & \cdots & * \\  & & & & \ddots & \vdots \\  & & & & & * \bmr\] We can see that $2)$ holds true by inspection. \\
    $2) \implies 3)$ Consider $Tv_h$ for $h \leq j$, by $2)$ we have $Tv_k \in \spa(\li vh) \subseteq \spa (\li vj)$. So $\spa (\li vj)$ is invariant under $T$. \\
    $3) \implies 2)$ Consider $Tv_j$, by $3)$ it is a linear combination of $\li vj$ because $Tv_j \in \spa(\li vj)$ so $\c M(T)(i,j) = 0$ if $i > j$.
\end{proof}
\begin{question}
    What about conditions for lower-triangular matrices?
\end{question}
\begin{theorem}
    Over $\b C$ every linear operator has an upper-triangular matrix representation.
\end{theorem}
\begin{lemma}
Over $\b C$, every linear operator has at least one eigenvalue.
\end{lemma}
\begin{proof}
    Take $v \in V\ \backslash \lb 0 \rb$, and consider the list $v, Tv, T^2v, \ldots, T^nv$ where $n = \dim V$. There is a nontrivial linear combination of these vectors which is $0$. Suppose the equation \[a_0v_1 + a_1Tv + a_2T^2v + \cdots + a_nT^nv = 0\]
    i.e. $p(T)v = 0$ for nonconstant $p(x) : = a_0 + a_1x + a_2x^2 + \cdots + a_nx^n$. By the fundamental theorem of algebra $p$ splits into linear factors over $\b C$.
    \[ p(x) = c(x - \lambda_1)(x - \lambda_2) \cdots (x - \lambda_m)\] for some $m \leq n$. Therefore 
    \[ p(T)v = c(T - \lambda_1 \b I)(T - \lambda_2  \b I) \cdots (T - \lambda_m \b I)\]
    Therefore at least one of these factors is not injective. This shows that $T$ has at least $1$ eigenvalue.
\end{proof}
\begin{theorem}
    For any $T \in \c L(V)$, $V$ is finite dimensional vector space over $\b C$, there exists its matrix representation $\c M(T)$ which is upper-triangular.
\end{theorem}
\begin{proof} 
    \textit{Base Step.} $n = 1$ is trivially true. \\
    \textit{Inductive Hypothesis.} Suppose Theorem holds for all vector spaces of dimension less than $\dim V$. \\
    \textit{Inductive Step.} Consider $\lambda \in \b C$ an eigenvalue of $T$ by lemma. We can define \[U := \range (T  -\lambda \b I)\] $U$ is a subspace of $V$. By the characterization of eigenvalues, $T - \lambda \b I$ is not surjective, hence $\range T - \lambda \b I \not\subseteq V$, hence $\dim \range (T - \lambda \b I) < \dim V$. Suppose $v \in \range (T - \lambda \b I)$, then $Tv = \underbrace{(T - \lambda \b I)v}_{\in U} + \underbrace{\lambda v}_{\in U}$ therefore we know that $U$ is invariant under $T$. Consider \[T/U \in L(U) : (T/U)v := Tv \forall v \in U\] If $U \neq \lb 0 \rb$, then there is a basis $\li um$ of $U$ ($m < n$) such that the matrix representation of $T/U$ with respect to $\li um$ is upper triangular by the inductive hypothesis. Extend $\li um$ to a basis of $V$, $\li um, \li vk$. We compute
    \[Tv_j = \underbrace{(T - \lambda \b I)v_j}_{\in U = \spa (\li um)} + \lambda v_j\] We also know that $Tu_l \in \spa (\li u{l-1})$. We can see the matrix representation and hence we are done
    \[ \begin{array}{cc}
         \\ \\ \\ \\ m \\ \\ \\ 
    \end{array}\left[\begin{array}{cccc|ccccccc}
         * & * & \cdots & * & * & * & * \\
         0 & * & \cdots & * & * & * & * \\
         \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots \\
         0 & 0 & \cdots & 0 & * & * & * \\
         \hline
         0 & 0 & \cdots & 0 & \lambda & 0 & 0 \\
         0 & 0 & \cdots & 0 & 0 & \lambda & 0 \\
         0 & 0 & \cdots & 0 & 0 & 0 & \lambda \\
    \end{array}\right]\]
\end{proof}
\begin{question}
    What about eigenvalues of a upper-triangular matrix?
\end{question}
\begin{theorem}
    An upper triangular matrix is invertible if and only if all its diagonal entries are nonzero.
\end{theorem}
\begin{proof}
    Suppose all diagonal entries are nonzero. Prove surjectivity. 
    \begin{align*}
        Tv_1  &=A_{1,1} v_1, A_{1,1} \neq 0 \implies v_1 \in \range T\\
     Tv_2  &=A_{1,2} v_1 + A_{2,2} v_2 , A_{2,2} \neq 0 \implies v_2 \in \range T \\
     \vdots \\
     Tv_n &= A_{1,n} v_1 + A_{2,n} v_2 + \cdots + A_{n,n} v_n, A_{n,n} \neq 0 \implies v_n \in \range T
    \end{align*} 
    Therefore $\range T = V$, so $T$ is surjective, hence $T$ is invertible. 
    Suppose at least one one diagonal entry is $0$ we want to show that $T$ is not invertible. Say $A_{j,j} = 0$ for some $j$ and upper triangular matrix $A$. If $j = 1$, then $v_1 \in \null T$, hence $T$ is not invertible, and we are done. If $j > 1$, consider $U := \spa (\li vj)$. $T$ maps $U$ to $\spa (\li v{j-1})$. This shows $T/U$ us not surjective, then we know that $T/U$ is not injective and there exists $u \in U$ such that $u \in \null T/U \implies u \in \null T$. Therefore $T$ is not injective. Hence $T$ is not invertible.
\end{proof}
\begin{corollary}
    An upper triangular matrix / operator in upper triangular form has the diagonal elements / entries as its eigenvalues.
\end{corollary}
\begin{example}The matrix
    \[ A = \bml 
5 & * & * & * & * & \\
0 & 9 & * & * & * & \\
0 & 0 & 1 & * & * & \\
0 & 0 & 0 & 8 & * & \\
0 & 0 & 0 & 0 & 10 & \\
\bmr\]
has eigenvalue $1,5,9,8,10$.
\end{example}
\begin{example}
    $T: \c P_n(\b R) \to \c P_n(\b R) : p \mapsto 3p'' - 5'p' + 7p$ has eigenvalues $3,-5,7$.
\end{example}
\begin{theorem}
    For $T \in \c L(V)$, where $V$ is a finite dimensional vector space, then the following are equivalent
    \begin{enumerate}
        \item $\c M(T)$ is diagonal.
        \item the corresponding basis for $V$ consists of eigenvalue of $T$.
        \item $V = U_1 \oplus U_2 \oplus \cdots \oplus U_n$ where $\dim U_j = 1$ and $U_j$ is invariant under $T$ for all $j$.
        \item $V = W_1 \oplus W_2 \oplus \cdots \oplus W_k$, where $V/W_l = \lambda_l \b I$ for all $l$ and $W_l$ is invariant under $T$.
        \item $\dim V = \dim W_1 + \dim W_2 + \cdots + W_k$, where $W_e = \null (T - \lambda_e \b I)$.
    \end{enumerate}
\end{theorem}

\begin{proof}
Trivial by matrix
    \[ \bml \lambda_1 & 0 & \cdots & 0 & \\
0 & \lambda_2 & \cdots & 0 & \\
\vdots & \vdots & \ddots & \vdots & \\
0 & 0 & \cdots & \lambda_k & \\ \bmr\]
\end{proof}
\subsection{Eigenspaces and Diagonal Matrices}
{\huge TODO}








